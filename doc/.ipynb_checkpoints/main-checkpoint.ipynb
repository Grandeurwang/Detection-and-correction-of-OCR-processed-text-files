{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GU4243/GR5243 Applied Data Science: Optical character recognition (OCR)\n",
    "\n",
    "### Group 6\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Optical character recognition (OCR) is the process of converting scanned images of machine printed or handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). The data workflow in a typical OCR system consists of three major stages:\n",
    "\n",
    "- Pre-processing\n",
    "- Word recognition \n",
    "- Post-processing\n",
    "\n",
    "In this project, we are going to focus on the third stage – post-processing, which includes two tasks: error detection and error correction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Error Detection\n",
    "Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or incorrectly processed words – check to see if an input string is a valid dictionary word or if its n-grams are all legal.\n",
    "\n",
    "In this project, We implement the probabilistic techniques – SVM garbage detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Labelling tesseact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "Applied Data Science \n",
    "Project 4 - Detection\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#import feature as feature\n",
    "\n",
    "\n",
    "def labelTesseract():\n",
    "        \n",
    "\n",
    "    \n",
    "    truth_files_list = glob.glob('../data/ground_truth/*.txt') \n",
    "    test_files_list = glob.glob('../data/tesseract/*.txt')\n",
    "\n",
    "    # only taking the ones that have the same number of lines in the file\n",
    "\n",
    "    truth_files = []\n",
    "    test_files = []\n",
    "    file_counts = 0 # store number of files (test and truth have same length)\n",
    "\n",
    "    for truth, test in zip(truth_files_list, test_files_list):\n",
    "        truth_length = len(open(truth).readlines())    \n",
    "        test_length = len(open(test).readlines())\n",
    "        if truth_length == test_length:\n",
    "            file_counts += 1\n",
    "            truth_files.append(truth)\n",
    "            test_files.append(test)\n",
    "\n",
    "\n",
    "    # only taking lines that have the same number of words\n",
    "    truth_words = []\n",
    "    test_words = []\n",
    "    truth_test_pair = [] # for correction\n",
    "    actual_counts = 0 # actual counts of numbers of words after filtering\n",
    "    for truth, test in zip(truth_files, test_files):\n",
    "            \n",
    "        with open(truth) as fd_truth:\n",
    "            with open(test) as fd_test:\n",
    "                for truth_line, test_line in zip(fd_truth, fd_test):\n",
    "                    tmp_truth = truth_line.strip().split()\n",
    "                    tmp_test = test_line.strip().split()\n",
    "                    if len(tmp_truth) == len(tmp_test):\n",
    "                        for truth_word, test_word in zip(tmp_truth, tmp_test):\n",
    "                            actual_counts += 1\n",
    "                            truth_words.append(truth_word)\n",
    "                            test_words.append(test_word)\n",
    "                            truth_test_pair.append((truth_word, test_word))\n",
    "    # uncomment below for testing\n",
    "    \n",
    "    # print(actual_counts)\n",
    "    print(len(truth_words))\n",
    "    print(len(test_words))\n",
    "    print(truth_words[:20])\n",
    "    print(test_words[:20])\n",
    "    \n",
    "    '''\n",
    "    # from the lists of words (truth, test) compare each of them\n",
    "    # label 1 if test is the same as truth (correct)\n",
    "    # label 0 if test is the different (wrong)\n",
    "\n",
    "    label_dict = defaultdict(int)\n",
    "\n",
    "    for truth, test in zip(truth_words, test_words):\n",
    "        if truth == test:\n",
    "            label_dict[test] = 1\n",
    "        else:\n",
    "            label_dict[test] = 0\n",
    "\n",
    "    print(label_dict)\n",
    "    '''\n",
    "\n",
    "    # due to not being able to store duplicates, switching to list\n",
    "\n",
    "    label = []\n",
    "    for truth, test in zip(truth_words, test_words):\n",
    "        if truth == test:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    \n",
    "    # uncomment below for commenting\n",
    "    \n",
    "    print(label[:20])\n",
    "    \n",
    "\n",
    "    return (truth_test_pair, test_words, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Divide into test/train (by default 20%, 80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_train(pair, label, k = 0.2):\n",
    "\n",
    "    # data = pd.DataFrame(words)\n",
    "    # split up data into k / 1-k percentage -- by defauly 80% train 20% test\n",
    "    train_data, test_data, train_label, test_label = train_test_split(pair, label, test_size = k)\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    X_train_truth = []\n",
    "    X_test_truth = []\n",
    "    for data in train_data:\n",
    "        X_train.append(data[1])\n",
    "        X_train_truth.append(data[0])\n",
    "    for data in test_data:\n",
    "        X_test.append(data[1])\n",
    "        X_test_truth.append(data[0])\n",
    "\n",
    "\n",
    "\n",
    "    return (X_train, X_test, train_label, test_label, X_train_truth, X_test_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatures(train_data, bigram_dict):\n",
    "    # f1\n",
    "    length = []\n",
    "    \n",
    "    # f2\n",
    "    v_count = []\n",
    "    c_count = []\n",
    "    v_div_l = []\n",
    "    c_div_l = []\n",
    "    v_div_c = []\n",
    "    \n",
    "    # f3\n",
    "    non_alnum = []\n",
    "    non_alnum_div_l = []\n",
    "    \n",
    "    # f4\n",
    "    digit = []\n",
    "    digit_l = []\n",
    "\n",
    "    # f5\n",
    "    lower = []\n",
    "    upper = []\n",
    "    lower_div_l = []\n",
    "    upper_div_l = []\n",
    "\n",
    "    #f6\n",
    "    three_consec_cons = []\n",
    "\n",
    "    #f7\n",
    "    alpha_num = []\n",
    "\n",
    "    #f8\n",
    "    six_consec_cons = []\n",
    "\n",
    "    #f9\n",
    "    infix = []\n",
    "\n",
    "    #f10\n",
    "    bigram = []\n",
    "\n",
    "    #f11\n",
    "    most_freq = []\n",
    "\n",
    "    #f12\n",
    "    non_div_alpha = []\n",
    "\n",
    "    for word in train_data:\n",
    "        length.append(f_1(word))\n",
    "        \n",
    "        v_count.append(f_2(word)[0])\n",
    "        c_count.append(f_2(word)[1])\n",
    "        v_div_l.append(f_2(word)[2])\n",
    "        c_div_l.append(f_2(word)[3])\n",
    "        v_div_c.append(f_2(word)[4])\n",
    "        \n",
    "        non_alnum.append(f_3(word)[0])\n",
    "        non_alnum_div_l.append(f_3(word)[1])\n",
    "\n",
    "        digit.append(f_4(word)[0])\n",
    "        digit_l.append(f_4(word)[1])\n",
    "\n",
    "        lower.append(f_5(word)[0])\n",
    "        upper.append(f_5(word)[1])\n",
    "        lower_div_l.append(f_5(word)[2])\n",
    "        upper_div_l.append(f_5(word)[3])\n",
    "\n",
    "        three_consec_cons.append(f_6(word))\n",
    "\n",
    "        alpha_num.append(f_7(word))\n",
    "\n",
    "        six_consec_cons.append(f_8(word))\n",
    "\n",
    "        infix.append(f_9(word))\n",
    "\n",
    "        # can change the scaling constant (third parameter)\n",
    "        bigram.append(f_10(word, bigram_dict, 10000))\n",
    "\n",
    "        most_freq.append(f_11(word))\n",
    "\n",
    "        non_div_alpha.append(f_12(word))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # create DataFrame\n",
    "\n",
    "    df = pd.DataFrame({'length': length,\n",
    "                       'num_vowels': v_count,\n",
    "                       'num_conso': c_count,\n",
    "                       'v_div_l': v_div_l,\n",
    "                       'c_div_l': c_div_l,\n",
    "                       'v_div_c': v_div_c,\n",
    "                       'non_alnum': non_alnum,\n",
    "                       'non_alnum_div_l': non_alnum_div_l,\n",
    "                       'digit': digit,\n",
    "                       'digit_l': digit_l,\n",
    "                       'lower': lower,\n",
    "                       'upper': upper,\n",
    "                       'lower_div_l': lower_div_l,\n",
    "                       'upper_div_l': upper_div_l,\n",
    "                       'three_consec_cons': three_consec_cons,\n",
    "                       'alpha_num': alpha_num,\n",
    "                       'six_consec_cons': six_consec_cons,\n",
    "                       'infix': infix,\n",
    "                       'bigram': bigram,\n",
    "                       'most_freq': most_freq,\n",
    "                       'non_div_alpha': non_div_alpha})\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def f_1(word):\n",
    "    \n",
    "    return len(word)\n",
    "\n",
    "def f_2(word):\n",
    "    l = len(word)\n",
    "    vowels = 'aeiou'\n",
    "    cons = 'bcdfghjklmnpqrstvwxyz'\n",
    "    v_count = 0\n",
    "    c_count = 0\n",
    "    \n",
    "    for c in word:\n",
    "        if c in vowels:\n",
    "            v_count += 1\n",
    "        elif c in cons:\n",
    "            c_count += 1\n",
    "\n",
    "\n",
    "    if c_count == 0:\n",
    "        return (v_count, c_count, v_count/l, c_count/l, 0)\n",
    "\n",
    "    return (v_count, c_count, v_count/l, c_count/l, v_count/c_count)\n",
    "\n",
    "def f_3(word):\n",
    "    l = len(word)\n",
    "    non_alnum = 0\n",
    "\n",
    "    for c in word:\n",
    "        if not c.isalnum():\n",
    "            non_alnum += 1\n",
    "\n",
    "    return (non_alnum, non_alnum/l)\n",
    "\n",
    "def f_4(word):\n",
    "    l = len(word)\n",
    "    digit = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isdigit():\n",
    "            digit += 1\n",
    "    return (digit, digit/l)\n",
    "\n",
    "def f_5(word):\n",
    "    l = len(word)\n",
    "    upper = 0 \n",
    "    lower = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isupper():\n",
    "            upper += 1\n",
    "        elif c.islower():\n",
    "            lower += 1 \n",
    "\n",
    "    return (lower, upper, lower/l, upper/l)\n",
    "\n",
    "def f_6(word):\n",
    "    l = len(word)\n",
    "    groups = groupby(word)\n",
    "    result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "\n",
    "    max_count = float('-inf')\n",
    "    for word_count in result:\n",
    "        if word_count[1] > max_count:\n",
    "            max_count = word_count[1]\n",
    "\n",
    "    if max_count >= 3:\n",
    "        return max_count/l\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_7(word):\n",
    "    l = len(word)\n",
    "    alnum = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isalnum():\n",
    "            alnum += 1\n",
    "    \n",
    "    non_alnum = l - alnum\n",
    "\n",
    "    if non_alnum > alnum:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_8(word):\n",
    "    cons = 'bcdfghjklmnpqrstvwxyz'\n",
    "    consec_cons = 0\n",
    "    max_count = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c in cons:\n",
    "            consec_cons += 1\n",
    "        else:\n",
    "            if max_count < consec_cons:\n",
    "                max_count = consec_cons\n",
    "            consec_cons = 0\n",
    "    if max_count == 0:\n",
    "        max_count = consec_cons\n",
    "\n",
    "    if max_count >= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_9(word):\n",
    "    infix = word[1:-1]\n",
    "    non_alnum = 0\n",
    "    \n",
    "    for c in infix:\n",
    "        if not c.isalnum():\n",
    "            non_alnum += 1\n",
    "    if non_alnum >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_10(word, bigram_dict, c = 10000):\n",
    "\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    naturalness = 0.0\n",
    "    for i in range(len(word)-1):\n",
    "        count += 1.0\n",
    "        naturalness += bigram_dict[(word[i], word[i+1])] / c\n",
    "\n",
    "    if count == 0.0:\n",
    "        return 0\n",
    "    return naturalness / count\n",
    "\n",
    "# return frequency of most frequent symbol\n",
    "def f_11(word):\n",
    "    l = len(word)\n",
    "    most_freq = collections.Counter(word).most_common(1)[0][1]\n",
    "\n",
    "    if most_freq >= 3:\n",
    "        return most_freq/l\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_12(word):\n",
    "    l = len(word)\n",
    "    alpha = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isalpha():\n",
    "            alpha += 1\n",
    "\n",
    "    non_alpha = l - alpha\n",
    "    if alpha == 0:\n",
    "        return 0\n",
    "    \n",
    "    return non_alpha / alpha\n",
    "\n",
    "def compute_bigram():\n",
    "    \n",
    "    bigram_dict = defaultdict(int)\n",
    "    truth_files_list = glob.glob('../data/ground_truth/*.txt')\n",
    "    for file in truth_files_list:\n",
    "        with open(file) as fd:\n",
    "            for line in fd:\n",
    "                each_line = line.strip().split()\n",
    "                for word in each_line:\n",
    "                    word = word.lower()\n",
    "                    for i in range(len(word)-1):\n",
    "                        bigram_dict[(word[i], word[i+1])] += 1\n",
    "\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Main SVM calling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221504\n",
      "221504\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC']\n",
      "['communlcatlons', 'network.', 'Member', 'companles', 'are', 'strongly', 'encouraged', 'to', 'provlde', 'thls', 'needed', 'support.', 'The', 'state', 'advocacy', 'program\"', '1nclud1ng', 'the', 'new', 'CMA/LINC']\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "['1ndustry', 'a', 'procedures', 'beneflts', 'all', 'are', 'market', 'and', '1n', 'polnts']\n",
      "['industry', 'a', 'procedures', 'benefits', 'all', 'are', 'market', 'and', 'in', 'points']\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "['requested', '1n', 'be', 'and', 'THE', '2]', 'prior', 'effectlveness', 'the', 'are']\n",
      "['requested', 'in', 'be', 'and', 'THE', 'j', 'prior', 'effectiveness', 'the', 'are']\n",
      "[1, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "pair, words, label = labelTesseract()\n",
    "train_data, test_data, train_label, test_label, ground_truth_train, ground_truth_test = div_train(pair, label)\n",
    "\n",
    "# uncomment to test for truth, tesseract pair\n",
    "\n",
    "print(train_data[:10])\n",
    "print(ground_truth_train[:10])\n",
    "print(train_label[:10])\n",
    "\n",
    "print(test_data[:10])\n",
    "print(ground_truth_test[:10])\n",
    "print(test_label[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output ground truth testing data to ground_truth_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(ground_truth_test).to_csv('../output/ground_truth_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]             data  label\n",
      "0       requested      1\n",
      "1              1n      0\n",
      "2              be      1\n",
      "3             and      1\n",
      "4             THE      1\n",
      "5              2]      0\n",
      "6           prior      1\n",
      "7   effectlveness      0\n",
      "8             the      1\n",
      "9             are      1\n",
      "10          Group      1\n",
      "11           com?      1\n",
      "12            31.      0\n",
      "13     Department      0\n",
      "14        755,500      0\n",
      "15             to      1\n",
      "16        provlde      1\n",
      "17          waste      1\n",
      "18              I      1\n",
      "19            For      1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.77     16717\n",
      "           1       0.85      0.90      0.87     27584\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     44301\n",
      "   macro avg       0.83      0.82      0.82     44301\n",
      "weighted avg       0.83      0.84      0.83     44301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_dict = compute_bigram()\n",
    "featureMatrix_train = buildFeatures(train_data, bigram_dict)\n",
    "featureMatrix_test = buildFeatures(test_data, bigram_dict)\n",
    "\n",
    "# uncomment for testing\n",
    "'''\n",
    "head = featureMatrix_train.head()\n",
    "print(head.to_string())\n",
    "'''\n",
    "\n",
    "# build classifier\n",
    "svm_class = SVC(kernel='rbf', verbose=True, gamma='scale', random_state = 123)\n",
    "svm_class.fit(featureMatrix_train, train_label)\n",
    "\n",
    "# prediction\n",
    "prediction = svm_class.predict(featureMatrix_test)\n",
    "\n",
    "output = pd.DataFrame({'data': test_data,\n",
    "                       'label': prediction})\n",
    "\n",
    "print(output[:20])\n",
    "\n",
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# print(confusion_matrix(test_label, prediction))\n",
    "print(classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output OCR to detected_typo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv('../output/detected_typo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Correction\n",
    "Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke a probabilistic estimate of the likelihood of the correction to rank order the candidates. \n",
    "\n",
    "In this project, our basic idea is using the Bayesian probability to choose the best candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import and Clean Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the test data from the detetion part as our training data. Import the detection output and the test ground truth data. Then clean the data, removing the numbers and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                1n\n",
       "5                2]\n",
       "7     effectlveness\n",
       "12              31.\n",
       "13       Department\n",
       "14          755,500\n",
       "20               by\n",
       "25        1n1tlated\n",
       "27           plants\n",
       "32    requlrements.\n",
       "Name: data, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_typo_and_correct = pd.read_csv('../output/detected_typo.csv',index_col = 0)\n",
    "# remove label column\n",
    "detected_typo = detected_typo_and_correct[detected_typo_and_correct.label == 0].data\n",
    "detected_correct = detected_typo_and_correct[detected_typo_and_correct.label == 1].data\n",
    "detected_typo_and_correct = detected_typo_and_correct.data\n",
    "detected_typo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_punct_num(series):\n",
    "    result = series.replace(r'\\d','')\n",
    "    result = result.str.extract(r'([a-zA-Z]+)').dropna()[0]\n",
    "    result = result.str.lower()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                 n\n",
       "7     effectlveness\n",
       "13       department\n",
       "20               by\n",
       "25                n\n",
       "27           plants\n",
       "32     requlrements\n",
       "34      leglslatlon\n",
       "38       nformatlon\n",
       "41         amerlcan\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_typo = remove_punct_num(detected_typo)\n",
    "# detected_typo_and_correct\n",
    "cleaned_typo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_typo_and_correct = remove_punct_num(detected_typo_and_correct)\n",
    "cleaned_detect_correct = remove_punct_num(detected_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_test = pd.read_csv('../output/ground_truth_test.csv', index_col = 0)\n",
    "cleaned_ground = remove_punct_num(pd.Series(ground_truth_test['0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define Necessary Functions and Important Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -1- Find Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set edit distance to be 1 and find candidates from all ground truth articles. There are 4 different situations:\n",
    "\n",
    "- Insertion\n",
    "- Deletion\n",
    "- Reversal\n",
    "- Substitution\n",
    "\n",
    "Because of the assumption in paper (there are only one typo in each word), we only find candidates that has one edit distance with the typo (there is a special case for reversal, since for the reversal case, edit distance is 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from nltk import edit_distance\n",
    "\n",
    "def typo_classification(typo,correct):\n",
    "    if (len(typo) > len(correct)):\n",
    "        return 'insertion'\n",
    "    elif (len(typo) < len(correct)):\n",
    "        return 'deletion'\n",
    "    else:\n",
    "        typo_count = Counter(typo)\n",
    "        correct_count = Counter(correct)\n",
    "        if typo_count == correct_count:\n",
    "            return 'reversal'\n",
    "        else:\n",
    "            return 'substitution'\n",
    "\n",
    "def find_candidates(typo,corpus):\n",
    "    candidates = []\n",
    "    candi_type = []\n",
    "    for word in corpus:\n",
    "        ed = edit_distance(typo,word)\n",
    "        word_type = typo_classification(typo,word)\n",
    "#         if len(typo) > 4:\n",
    "#             if ed in [1,2]:\n",
    "#                 candidates.append(word)\n",
    "#                 candi_type.append(word_type)\n",
    "#         else:\n",
    "        if ((ed == 1) |((ed == 2) & (word_type == 'reversal'))):\n",
    "            candidates.append(word)\n",
    "            candi_type.append(word_type)\n",
    "    return candidates,candi_type\n",
    "\n",
    "def find_position(typo,candidates):\n",
    "    position = []\n",
    "    for corr in candidates:\n",
    "        typo_type = typo_classification(typo,corr)\n",
    "        \n",
    "        if (typo_type == 'deletion'):\n",
    "            typo += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    if corr[i] != corr[i-1]:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        position.append([typo,corr,\"#\",corr[i],i-1,typo_type])\n",
    "                        break\n",
    "                        \n",
    "                i += 1\n",
    "        elif (typo_type == 'insertion'):\n",
    "            corr += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    \n",
    "                    if typo[i] != typo[i-1]:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        break\n",
    "                    elif ((typo[i] == typo[i-1])& (typo[i] == typo[i-2])):\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-2,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        break\n",
    "                i += 1\n",
    "        elif (typo_type == 'substitution'):\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    position.append([typo,corr,typo[i],corr[i],i,typo_type])\n",
    "                    break\n",
    "                i+=1\n",
    "                \n",
    "        elif (typo_type == 'reversal'):\n",
    "            i = 0\n",
    "            while i < len(corr)-1:\n",
    "                if ((typo[i] == corr[i+1]) & (typo[i+1] == corr[i])):\n",
    "                    typo_comb = typo[i] + typo[i+1]\n",
    "                    position.append([typo,corr,typo_comb,typo_comb[::-1],i,typo_type])\n",
    "                    break\n",
    "                i +=1\n",
    "    return position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -2- Define N & V\n",
    "N is the word size and V is the vocabulary size in the training set, which is for prior calculation later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "truth_counts = 0\n",
    "training = []\n",
    "# create a list of all .txt files\n",
    "truth_files_list = glob.glob('../data/ground_truth/*.txt')\n",
    "# reading the ground truth file\n",
    "for file in truth_files_list:\n",
    "    with open(file) as fd:\n",
    "        for line in fd:\n",
    "            each_line = re.findall(r\"[\\w']+\",line)\n",
    "            for word in each_line:\n",
    "                training.append(word)\n",
    "                truth_counts += 1\n",
    "                \n",
    "training = pd.Series(training)\n",
    "training = training.str.replace(r'\\d','').dropna()\n",
    "\n",
    "training = training.str.lower()\n",
    "training = training[training != '']\n",
    "corpus = training.unique()\n",
    "\n",
    "N = len(training)\n",
    "V = len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -3- Count bigram & 1gram & freq Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating word(character) frequency for ground truth paper for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "def bigram(string):\n",
    "    x = []\n",
    "    for i in range(len(string)):\n",
    "        if i == len(string) - 1:\n",
    "            return x\n",
    "        else:\n",
    "            x.append(string[i] + string[i+1])\n",
    "            \n",
    "def one_gram(string):\n",
    "    return list(string)\n",
    "\n",
    "def total_freq(training,types):\n",
    "    if types == 'bigram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += bigram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'onegram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += one_gram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'freq':\n",
    "        return Counter(training)\n",
    "    \n",
    "total_freq_bigram = total = total_freq(training,types = 'bigram')\n",
    "total_freq_1gram = total = total_freq(training,types = 'onegram')\n",
    "total_freq = total = total_freq(training,types = 'freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -4- Confusion Matrics\n",
    "Import the confusion matrics for likelihood calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionsub=pd.read_csv('../data/confusion_matrix/sub_matrix.csv',index_col = 0)\n",
    "confusionadd=pd.read_csv('../data/confusion_matrix/add_matrix.csv',index_col = 0)\n",
    "confusiondel=pd.read_csv('../data/confusion_matrix/del_matrix.csv',index_col = 0)\n",
    "confusionrev=pd.read_csv('../data/confusion_matrix/rev_matrix.csv',index_col = 0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -5- Likelihood Calculation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is the algorithm of our correction part form paper(C3):\n",
    "\n",
    "- del[cp-1, cp]/chars[cp-1, cp] if deletion;\n",
    "\n",
    "- add[cp-l, tp]/chars[cp-l]  if insertion;\n",
    "\n",
    "- sub[tp, Cp]/chars[cp] if substitution;\n",
    "\n",
    "- rev[cp, cp + l]/chars[cp, cp + 1] if reversal.\n",
    "\n",
    "We should be careful when we deal with the cases then the index of position of correction equal to 0 since in these cases, we don't have any information about cp-1 and according to the advice of professor. \n",
    "We calculate the number of words in training set instead which is also quite rational.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction = pd.DataFrame()\n",
    "\n",
    "def probabilityfunction(correction):\n",
    "    for i in range(0,correction.shape[0]):\n",
    "        typo = correction.iloc[i,0]\n",
    "        index=correction.iloc[i,4]\n",
    "        specificword=correction.iloc[i,1]\n",
    "        if correction.iloc[i,5]=='insertion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=typo[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result =add/total\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total=len(training)\n",
    "\n",
    "                result=add/total\n",
    "\n",
    "        if correction.iloc[i,5]=='deletion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result=delt/total\n",
    "\n",
    "\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                totall=len(training)\n",
    "\n",
    "                result=delt/totall\n",
    "        if correction.iloc[i,5]=='reversal':\n",
    "\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index]\n",
    "                Y=specificword[index+1]\n",
    "                rev=confusionrev.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                result=rev/total\n",
    "\n",
    "\n",
    "        if correction.iloc[i,5]=='substitution':\n",
    "            X=correction.iloc[i,2]\n",
    "            Y=correction.iloc[i,3]\n",
    "            sub = confusionsub.loc[X,Y]\n",
    "\n",
    "            total = total_freq_1gram[Y]\n",
    "                #lis.append(total)\n",
    "            result=sub/total\n",
    "            \n",
    "        correction.loc[i,'probability of t given c'] = result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -6- Posterior Calculation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Bayesian combination rule to score each candidate correction and choose the candidate with highest score to be our correction.\n",
    "\n",
    "The method we use to calcuate pr(c) is ELE and the posterior probility = pr(c)*pr(t|c).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correction(typos):\n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "    output = []\n",
    "    no_correction = 0\n",
    "    no_correct_word = []\n",
    "\n",
    "    for typo in tqdm_notebook(typos):\n",
    "        try:\n",
    "            candidates,cand_type = find_candidates(typo,corpus)\n",
    "            correction = find_position(typo,candidates)\n",
    "            correction = pd.DataFrame(correction)\n",
    "\n",
    "            if correction.empty:  \n",
    "                output.append(typo)\n",
    "                no_correct_word.append(typo)\n",
    "                no_correction += 1\n",
    "\n",
    "            else:\n",
    "                correction.columns = ['Typo','Correction','old','new','index','type']\n",
    "                correction = correction[correction['index'] >= 0]\n",
    "\n",
    "                if len(correction) == 1:\n",
    "                    output.append(correction.loc[0,'Correction'])\n",
    "                else:\n",
    "                    # 1. calculate the prior\n",
    "\n",
    "                    freq = [] # the number of times that the proposed correction c appears in the training set\n",
    "                    for cor in correction['Correction']:\n",
    "                        freq.append(total_freq[cor])    \n",
    "\n",
    "                    N = len(training) # the word size\n",
    "                    V = len(corpus) # the vocabulary size\n",
    "\n",
    "                    prior = (pd.DataFrame(freq) + 0.5)/(N + V/2)\n",
    "\n",
    "                    correction['probability of c'] = prior\n",
    "                    \n",
    "                    # 2. Calculate the likelihood\n",
    "                    \n",
    "                    probabilityfunction(correction)\n",
    "\n",
    "                    # 3. Calculate the posterior and find the correction that has maximum posterior\n",
    "\n",
    "                    correction['posterior'] = correction['probability of c'] * correction['probability of t given c']\n",
    "                    best = correction[correction.posterior == correction.posterior.max()].Correction.values[0]\n",
    "                    output.append(best)\n",
    "        except:\n",
    "            output.append(typo)\n",
    "            no_correct_word.append(typo)\n",
    "            no_correction += 1\n",
    "    \n",
    "    return (pd.Series(output)),no_correction,no_correct_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Correct Typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the final posterior calculation function above, we import the typos and do the correction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcdf6fa95814016a50d4c33f31fc227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12884), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ying/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "typos = cleaned_typo\n",
    "Correction_output_all,no_correction_num,no_correct_word = Correction(typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrected rate: 40.1%\n"
     ]
    }
   ],
   "source": [
    "print('No corrected rate: {:.1%}'.format(no_correction_num/len(Correction_output_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output correction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0                n\n",
       "1    effectiveness\n",
       "2      departments\n",
       "3               be\n",
       "4                n\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correction_output_all.to_csv('../output/Correction_output_all.csv')\n",
    "Correction_output = pd.read_csv('../output/Correction_output_all.csv',index_col = 0)\n",
    "Correction_output = pd.Series(output[1])\n",
    "Correction_output.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,\n",
    "\n",
    "- precision = number of correct items / number of items in OCR output\n",
    "\n",
    "- recall = number of correct items / number of items in ground truth\n",
    "\n",
    "where items refer to either characters or words, and ground truth is the original text stored in the plain text file.\n",
    "\n",
    "Both precision and recall are mathematically convenient measures because their numeric values are some decimal fractions in the range between 0.0 and 1.0, and thus can be written as percentages. For instance, recall is the percentage of words in the original text correctly found by the OCR engine, whereas precision is the percentage of correctly found words with respect to the total word count of the OCR output. Note that in the OCR-related literature, the term OCR accuracy often refers to recall.\n",
    "\n",
    "We do the evaluation part in both word level and the character-level. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define Exact Intersection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def vintersection(list1,list2,ngram = False):\n",
    "    list1_dict = {}\n",
    "    list2_dict = {}\n",
    "    \n",
    "    if ngram:\n",
    "        list1 = list(''.join(list1))\n",
    "        list2 = list(''.join(list2))\n",
    "\n",
    "    for i in list1:\n",
    "        list1_dict[i] = list1_dict.get(i,0) + 1\n",
    "\n",
    "    for i in list2:\n",
    "        list2_dict[i] = list2_dict.get(i,0) + 1\n",
    "        \n",
    "    result = {}\n",
    "    for key in list1_dict.keys():\n",
    "        if key in list2_dict.keys():\n",
    "            value1 = list1_dict[key]\n",
    "            value2 = list2_dict[key]\n",
    "            min_value = min(value1,value2)\n",
    "            result[key] = min_value\n",
    "    return sum(result.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Calculate Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesseract\n",
    "precision = vintersection(cleaned_typo_and_correct,cleaned_ground)/len(cleaned_typo_and_correct)\n",
    "recall = vintersection(cleaned_typo_and_correct,cleaned_ground)/len(cleaned_ground)\n",
    "precision_n = vintersection(cleaned_typo_and_correct,cleaned_ground, ngram = True)/len(''.join(list(cleaned_typo_and_correct)))\n",
    "recall_n = vintersection(cleaned_typo_and_correct,cleaned_ground, ngram = True)/len(''.join(list(cleaned_ground)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing\n",
    "result = Correction_output.append(cleaned_detect_correct)\n",
    "precision_post_all = vintersection(result,cleaned_ground)/len(result)\n",
    "recall_post_all = vintersection(result,cleaned_ground)/len(cleaned_ground)\n",
    "precision_post_all_n = vintersection(result,cleaned_ground, ngram = True)/len(''.join(list(result)))\n",
    "recall_post_all_n = vintersection(result,cleaned_ground, ngram = True)/len(''.join(list(cleaned_ground)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 The Result Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tesseract</th>\n",
       "      <th>Post Processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision_word_level</th>\n",
       "      <td>0.670116</td>\n",
       "      <td>0.769737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_word_level</th>\n",
       "      <td>0.656665</td>\n",
       "      <td>0.754286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_character_level</th>\n",
       "      <td>0.940102</td>\n",
       "      <td>0.961398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_character_level</th>\n",
       "      <td>0.912396</td>\n",
       "      <td>0.935077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Tesseract  Post Processing\n",
       "precision_word_level        0.670116         0.769737\n",
       "recall_word_level           0.656665         0.754286\n",
       "precision_character_level   0.940102         0.961398\n",
       "recall_character_level      0.912396         0.935077"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame()\n",
    "table['Tesseract'] = [precision, recall,precision_n,recall_n]\n",
    "table['Post Processing'] = [precision_post_all,recall_post_all, precision_post_all_n,recall_post_all_n]\n",
    "#table['PostProcessing2'] = [precision_post_oneerror,recall_post_oneerror]\n",
    "table.index = ['precision_word_level', 'recall_word_level','precision_character_level','recall_character_level']\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
