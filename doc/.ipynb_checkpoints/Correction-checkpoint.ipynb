{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from detection import *\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair, words, label = labelTesseract()\n",
    "train_data, test_data, train_label, test_label, ground_truth_train, ground_truth_test = div_train(pair, label)\n",
    "\n",
    "# uncomment to test for truth, tesseract pair\n",
    "'''\n",
    "print(train_data[:10])\n",
    "print(ground_truth_train[:10])\n",
    "print(train_label[:10])\n",
    "\n",
    "print(test_data[:10])\n",
    "print(ground_truth_test[:10])\n",
    "print(test_label[:10])\n",
    "'''\n",
    "\n",
    "\n",
    "bigram_dict = compute_bigram()\n",
    "featureMatrix_train = buildFeatures(train_data, bigram_dict)\n",
    "featureMatrix_test = buildFeatures(test_data, bigram_dict)\n",
    "\n",
    "# uncomment for testing\n",
    "'''\n",
    "head = featureMatrix_train.head()\n",
    "print(head.to_string())\n",
    "'''\n",
    "\n",
    "# build classifier\n",
    "svm_class = SVC(kernel='rbf', verbose=True, gamma='scale')\n",
    "svm_class.fit(featureMatrix_train, train_label)\n",
    "\n",
    "# prediction\n",
    "prediction = svm_class.predict(featureMatrix_test)\n",
    "\n",
    "output = pd.DataFrame({'data': test_data,\n",
    "                       'label': prediction})\n",
    "\n",
    "print(output[:20])\n",
    "\n",
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(test_label, prediction))\n",
    "print(classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Output OCR to detected_typo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv('../output/detected_typo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Import detected typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$50,000.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1nclud1ng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>members</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>process</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DDT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thls</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>new</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>another</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>polltlcal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Callahan,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pennsylvanla,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>more</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dlscuss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>are</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             data  label\n",
       "0        $50,000.      0\n",
       "1       1nclud1ng      0\n",
       "2         members      1\n",
       "3             29,      0\n",
       "4             can      1\n",
       "5         process      1\n",
       "6             DDT      1\n",
       "7            thls      0\n",
       "8             new      1\n",
       "9         another      1\n",
       "10      polltlcal      0\n",
       "11           from      1\n",
       "12      Callahan,      1\n",
       "13            and      1\n",
       "14           5710      0\n",
       "15  Pennsylvanla,      0\n",
       "16            the      1\n",
       "17           more      1\n",
       "18        dlscuss      0\n",
       "19            are      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_typo = pd.read_csv('../output/detected_typo.csv',index_col = 0)\n",
    "detected_typo.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Clean detected typo (remove punctuation & number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      $50,000.\n",
       "1     1nclud1ng\n",
       "3           29,\n",
       "7          thls\n",
       "10    polltlcal\n",
       "Name: data, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_typo_and_correct = pd.read_csv('../output/detected_typo.csv',index_col = 0)\n",
    "# remove label column\n",
    "detected_typo = detected_typo_and_correct[detected_typo_and_correct.label == 0].data\n",
    "detected_typo_and_correct = detected_typo_and_correct.data\n",
    "detected_typo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_punct_num(series):\n",
    "    result = series.replace(r'\\d','')\n",
    "    result = result.str.extract(r'([a-zA-Z]+)').dropna()[0]\n",
    "    result = result.str.lower()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_typo = remove_punct_num(detected_typo)\n",
    "cleaned_typo_and_correct = remove_punct_num(detected_typo_and_correct)\n",
    "# detected_typo_and_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair, words, label = labelTesseract()\n",
    "\n",
    "true_typo = pd.DataFrame(pair)\n",
    "true_typo.columns = ['correct','typo']\n",
    "for col in true_typo.columns:\n",
    "    true_typo[col] = remove_punct_num(true_typo[col])\n",
    "true_typo = true_typo[true_typo['correct'] != true_typo['typo']].dropna().reset_index(drop = True)\n",
    "true_typo.drop_duplicates(keep = 'first',inplace = True)\n",
    "true_typo = true_typo[['typo','correct']].reset_index(drop = True)\n",
    "# true_typo.set_index('typo',inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define N & V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "truth_counts = 0\n",
    "training = []\n",
    "# create a list of all .txt files\n",
    "truth_files_list = glob.glob('../data/ground_truth/*.txt')\n",
    "# reading the ground truth file\n",
    "for file in truth_files_list:\n",
    "    with open(file) as fd:\n",
    "        for line in fd:\n",
    "            each_line = re.findall(r\"[\\w']+\",line)\n",
    "            for word in each_line:\n",
    "                training.append(word)\n",
    "                truth_counts += 1\n",
    "                \n",
    "training = pd.Series(training)\n",
    "training = training.str.replace(r'\\d','').dropna()\n",
    "\n",
    "training = training.str.lower()\n",
    "training = training[training != '']\n",
    "corpus = training.unique()\n",
    "\n",
    "N = len(training)\n",
    "V = len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Find Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "from nltk import edit_distance\n",
    "\n",
    "def typo_classification(typo,correct):\n",
    "    if (len(typo) > len(correct)):\n",
    "        return 'insertion'\n",
    "    elif (len(typo) < len(correct)):\n",
    "        return 'deletion'\n",
    "    else:\n",
    "        typo_count = Counter(typo)\n",
    "        correct_count = Counter(correct)\n",
    "        if typo_count == correct_count:\n",
    "            return 'reversal'\n",
    "        else:\n",
    "            return 'substitution'\n",
    "\n",
    "def find_candidates(typo,corpus):\n",
    "    candidates = []\n",
    "    candi_type = []\n",
    "    for word in corpus:\n",
    "        ed = edit_distance(typo,word)\n",
    "        word_type = typo_classification(typo,word)\n",
    "#         if len(typo) > 4:\n",
    "#             if ed in [1,2]:\n",
    "#                 candidates.append(word)\n",
    "#                 candi_type.append(word_type)\n",
    "#         else:\n",
    "        if ((ed == 1) |((ed == 2) & (word_type == 'reversal'))):\n",
    "            candidates.append(word)\n",
    "            candi_type.append(word_type)\n",
    "    return candidates,candi_type\n",
    "\n",
    "def find_position(typo,candidates):\n",
    "    position = []\n",
    "    for corr in candidates:\n",
    "        typo_type = typo_classification(typo,corr)\n",
    "        \n",
    "        if (typo_type == 'deletion'):\n",
    "            typo += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    if corr[i] != corr[i-1]:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        position.append([typo,corr,\"#\",corr[i],i-1,typo_type])\n",
    "                        break\n",
    "                        \n",
    "                i += 1\n",
    "        elif (typo_type == 'insertion'):\n",
    "            corr += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    \n",
    "                    if typo[i] != typo[i-1]:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        break\n",
    "                    elif ((typo[i] == typo[i-1])& (typo[i] == typo[i-2])):\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-2,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        break\n",
    "                i += 1\n",
    "        elif (typo_type == 'substitution'):\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    position.append([typo,corr,typo[i],corr[i],i,typo_type])\n",
    "                    break\n",
    "                i+=1\n",
    "                \n",
    "        elif (typo_type == 'reversal'):\n",
    "            i = 0\n",
    "            while i < len(corr)-1:\n",
    "                if ((typo[i] == corr[i+1]) & (typo[i+1] == corr[i])):\n",
    "                    typo_comb = typo[i] + typo[i+1]\n",
    "                    position.append([typo,corr,typo_comb,typo_comb[::-1],i,typo_type])\n",
    "                    break\n",
    "                i +=1\n",
    "    return position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Import 4 confusion matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionsub=pd.read_csv('../data/confusion_matrix/sub_matrix.csv',index_col = 0)\n",
    "confusionadd=pd.read_csv('../data/confusion_matrix/add_matrix.csv',index_col = 0)\n",
    "confusiondel=pd.read_csv('../data/confusion_matrix/del_matrix.csv',index_col = 0)\n",
    "confusionrev=pd.read_csv('../data/confusion_matrix/rev_matrix.csv',index_col = 0) \n",
    "# corpus = set(truth_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Count bigram & 1gram & freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "def bigram(string):\n",
    "    x = []\n",
    "    for i in range(len(string)):\n",
    "        if i == len(string) - 1:\n",
    "            return x\n",
    "        else:\n",
    "            x.append(string[i] + string[i+1])\n",
    "            \n",
    "def one_gram(string):\n",
    "    return list(string)\n",
    "\n",
    "def total_freq(training,types):\n",
    "    if types == 'bigram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += bigram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'onegram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += one_gram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'freq':\n",
    "        return Counter(training)\n",
    "    \n",
    "total_freq_bigram = total = total_freq(training,types = 'bigram')\n",
    "total_freq_1gram = total = total_freq(training,types = 'onegram')\n",
    "total_freq = total = total_freq(training,types = 'freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Calculate Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction = pd.DataFrame()\n",
    "\n",
    "def probabilityfunction(correction):\n",
    "    for i in range(0,correction.shape[0]):\n",
    "        typo = correction.iloc[i,0]\n",
    "        index=correction.iloc[i,4]\n",
    "        specificword=correction.iloc[i,1]\n",
    "        if correction.iloc[i,5]=='insertion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=typo[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result =add/total\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total=len(training)\n",
    "\n",
    "                result=add/total\n",
    "\n",
    "        if correction.iloc[i,5]=='deletion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result=delt/total\n",
    "\n",
    "\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                totall=len(training)\n",
    "\n",
    "                result=delt/totall\n",
    "        if correction.iloc[i,5]=='reversal':\n",
    "\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index]\n",
    "                Y=specificword[index+1]\n",
    "                rev=confusionrev.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                result=rev/total\n",
    "\n",
    "\n",
    "        if correction.iloc[i,5]=='substitution':\n",
    "            X=correction.iloc[i,2]\n",
    "            Y=correction.iloc[i,3]\n",
    "            sub = confusionsub.loc[X,Y]\n",
    "\n",
    "            total = total_freq_1gram[Y]\n",
    "                #lis.append(total)\n",
    "            result=sub/total\n",
    "            \n",
    "        correction.loc[i,'probability of t given c'] = result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Calculate Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correction(typos):\n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "    output = []\n",
    "    no_correction = 0\n",
    "    no_correct_word = []\n",
    "\n",
    "    for typo in tqdm_notebook(typos):\n",
    "        try:\n",
    "            candidates,cand_type = find_candidates(typo,corpus)\n",
    "            correction = find_position(typo,candidates)\n",
    "            correction = pd.DataFrame(correction)\n",
    "\n",
    "            if correction.empty:  \n",
    "                output.append(typo)\n",
    "                no_correct_word.append(typo)\n",
    "                no_correction += 1\n",
    "\n",
    "            else:\n",
    "                correction.columns = ['Typo','Correction','old','new','index','type']\n",
    "                correction = correction[correction['index'] >= 0]\n",
    "\n",
    "                if len(correction) == 1:\n",
    "                    output.append(correction.loc[0,'Correction'])\n",
    "                else:\n",
    "                    # 1. calculate the prior\n",
    "\n",
    "                    freq = [] # the number of times that the proposed correction c appears in the training set\n",
    "                    for cor in correction['Correction']:\n",
    "                        freq.append(total_freq[cor])    \n",
    "\n",
    "                    N = len(training)\n",
    "                    V = len(corpus)\n",
    "\n",
    "                    prior = (pd.DataFrame(freq) + 0.5)/(N + V/2)\n",
    "\n",
    "                    correction['probability of c'] = prior\n",
    "\n",
    "                    probabilityfunction(correction)\n",
    "\n",
    "                    # 3. Calculate the posterior and find the correction that has maximum posterior\n",
    "\n",
    "                    correction['posterior'] = correction['probability of c'] * correction['probability of t given c']\n",
    "                    best = correction[correction.posterior == correction.posterior.max()].Correction.values[0]\n",
    "                    output.append(best)\n",
    "        except:\n",
    "#             print(typo)\n",
    "            output.append(typo)\n",
    "            no_correct_word.append(typo)\n",
    "            no_correction += 1\n",
    "    #         break\n",
    "    \n",
    "    return (pd.Series(output)),no_correction,no_correct_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_denominator = len(cleaned_typo)\n",
    "recall_denominator = len(cleaned_typo_and_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def vintersection(list1,list2,ngram = False):\n",
    "    list1_dict = {}\n",
    "    list2_dict = {}\n",
    "    \n",
    "    if ngram:\n",
    "        list1 = list(''.join(list1))\n",
    "        list2 = list(''.join(list2))\n",
    "\n",
    "    for i in list1:\n",
    "        list1_dict[i] = list1_dict.get(i,0) + 1\n",
    "\n",
    "    for i in list2:\n",
    "        list2_dict[i] = list2_dict.get(i,0) + 1\n",
    "        \n",
    "    result = {}\n",
    "    for key in list1_dict.keys():\n",
    "        if key in list2_dict.keys():\n",
    "            value1 = list1_dict[key]\n",
    "            value2 = list2_dict[key]\n",
    "            min_value = min(value1,value2)\n",
    "            result[key] = min_value\n",
    "    return sum(result.values())\n",
    "\n",
    "def precision(GT,OCR,ngram = False):\n",
    "    TP = vintersection(GT,OCR,ngram)\n",
    "    if ngram:\n",
    "        OCR = list(''.join(OCR))\n",
    "    return TP/len(OCR)\n",
    "\n",
    "def recall(GT,OCR,ngram = False):\n",
    "    TP = vintersection(GT,OCR)\n",
    "    if ngram:\n",
    "        GT = list(''.join(GT))\n",
    "    return TP/len(GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Case 1: Correct all typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d142cdd6f9d24ccda4f2c02602ffb6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12111), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "typos = true_typo['typo']\n",
    "correct = true_typo['correct']\n",
    "\n",
    "Correction_output,no_correction_num,no_correct_word = Correction(typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2%}'.format(vintersection(Correction_output,correct)/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No corrected rate: {:.1%}'.format(no_correction_num/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output correction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correction_output.to_csv('Correction_output_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Recall & precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(Correction_output,correct[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     communlcations\n",
       "1          companies\n",
       "2            provide\n",
       "3               this\n",
       "4             includ\n",
       "5            heavily\n",
       "6           involved\n",
       "7                  n\n",
       "8           crltlcal\n",
       "9      environmental\n",
       "10            issues\n",
       "11              sent\n",
       "12          national\n",
       "13       legislators\n",
       "14          disposal\n",
       "15              bill\n",
       "16          continue\n",
       "17                 m\n",
       "18         suparfund\n",
       "19             which\n",
       "20          detailed\n",
       "21               cma\n",
       "22     rlghtitoiknow\n",
       "23            action\n",
       "24         continues\n",
       "25             toxic\n",
       "26        prevention\n",
       "27          requires\n",
       "28         reporting\n",
       "29        nformatlon\n",
       "           ...      \n",
       "70                mm\n",
       "71          analysis\n",
       "72         headlines\n",
       "73              june\n",
       "74          reauthor\n",
       "75            gained\n",
       "76         prlaarlly\n",
       "77          ntruslon\n",
       "78               nto\n",
       "79    reconclllatlon\n",
       "80              from\n",
       "81         edltorlal\n",
       "82             might\n",
       "83       antlclpated\n",
       "84         following\n",
       "85          accident\n",
       "86           intense\n",
       "87           inltlal\n",
       "88          interest\n",
       "89         potential\n",
       "90               nci\n",
       "91            united\n",
       "92           stories\n",
       "93         questions\n",
       "94            allude\n",
       "95          adequacy\n",
       "96          exlstlng\n",
       "97         institute\n",
       "98          question\n",
       "99             their\n",
       "Length: 100, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correction_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Only consider edit distance = 1 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_typo.map(true_typo_dict)\n",
    "# cleaned_typo\n",
    "ed_1_typo = []\n",
    "from nltk import edit_distance\n",
    "for i in range(len(true_typo)):\n",
    "    typo = true_typo.loc[i,'typo']\n",
    "    correct = true_typo.loc[i,'correct']\n",
    "    if edit_distance(typo,correct) == 1:\n",
    "        ed_1_typo.append([typo,correct])\n",
    "                         \n",
    "ed_1_typo_df = pd.DataFrame(ed_1_typo)\n",
    "ed_1_typo_df.columns = ['typo','correct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b35598cc944c81b82b6045a35ecb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "typos = ed_1_typo_df['typo']\n",
    "correct = ed_1_typo_df['correct']\n",
    "\n",
    "Correction_output,no_correction_num,no_correct_word = Correction(typos[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrected rate: 2.5%\n"
     ]
    }
   ],
   "source": [
    "print('No corrected rate: {:.1%}'.format(no_correction_num/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(vintersection(Correction_output,correct[:200])/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correction_output.to_csv('Correction_output_ed_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Recall & precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vintersection(Correction_output,correct)/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
