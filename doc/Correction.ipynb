{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from detection import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "Applied Data Science \n",
    "Project 4 - Detection\n",
    "\n",
    "'''\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "import nltk\n",
    "\n",
    "#import feature as feature\n",
    "\n",
    "\n",
    "def labelTesseract():\n",
    "        \n",
    "\n",
    "    \n",
    "    truth_files_list = glob.glob('../data/ground_truth/*.txt') \n",
    "    test_files_list = glob.glob('../data/tesseract/*.txt')\n",
    "\n",
    "    # only taking the ones that have the same number of lines in the file\n",
    "\n",
    "    truth_files = []\n",
    "    test_files = []\n",
    "    file_counts = 0 # store number of files (test and truth have same length)\n",
    "\n",
    "    for truth, test in zip(truth_files_list, test_files_list):\n",
    "        truth_length = len(open(truth).readlines())    \n",
    "        test_length = len(open(test).readlines())\n",
    "        if truth_length == test_length:\n",
    "            file_counts += 1\n",
    "            truth_files.append(truth)\n",
    "            test_files.append(test)\n",
    "\n",
    "\n",
    "    # only taking lines that have the same number of words\n",
    "    truth_words = []\n",
    "    test_words = []\n",
    "    truth_test_pair = [] # for correction\n",
    "    actual_counts = 0 # actual counts of numbers of words after filtering\n",
    "    for truth, test in zip(truth_files, test_files):\n",
    "            \n",
    "        with open(truth) as fd_truth:\n",
    "            with open(test) as fd_test:\n",
    "                for truth_line, test_line in zip(fd_truth, fd_test):\n",
    "                    tmp_truth = truth_line.strip().split()\n",
    "                    tmp_test = test_line.strip().split()\n",
    "                    if len(tmp_truth) == len(tmp_test):\n",
    "                        for truth_word, test_word in zip(tmp_truth, tmp_test):\n",
    "                            actual_counts += 1\n",
    "                            truth_words.append(truth_word)\n",
    "                            test_words.append(test_word)\n",
    "                            truth_test_pair.append((truth_word, test_word))\n",
    "    # uncomment below for testing\n",
    "    ''' \n",
    "    print(actual_counts)\n",
    "    print(len(truth_words))\n",
    "    print(len(test_words))\n",
    "    print(truth_words[:20])\n",
    "    print(test_words[:20])\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # from the lists of words (truth, test) compare each of them\n",
    "    # label 1 if test is the same as truth (correct)\n",
    "    # label 0 if test is the different (wrong)\n",
    "\n",
    "    label_dict = defaultdict(int)\n",
    "\n",
    "    for truth, test in zip(truth_words, test_words):\n",
    "        if truth == test:\n",
    "            label_dict[test] = 1\n",
    "        else:\n",
    "            label_dict[test] = 0\n",
    "\n",
    "    print(label_dict)\n",
    "    '''\n",
    "\n",
    "    # due to not being able to store duplicates, switching to list\n",
    "\n",
    "    label = []\n",
    "    for truth, test in zip(truth_words, test_words):\n",
    "        if truth == test:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    \n",
    "    # uncomment below for commenting\n",
    "    '''\n",
    "    print(label[:20])\n",
    "    '''\n",
    "\n",
    "    return (truth_test_pair, test_words, label)\n",
    "\n",
    "def div_train(pair, label, k = 0.2):\n",
    "\n",
    "    # data = pd.DataFrame(words)\n",
    "    # split up data into k / 1-k percentage -- by defauly 80% train 20% test\n",
    "    train_data, test_data, train_label, test_label = train_test_split(pair, label, test_size = k)\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    X_train_truth = []\n",
    "    X_test_truth = []\n",
    "    for data in train_data:\n",
    "        X_train.append(data[1])\n",
    "        X_train_truth.append(data[0])\n",
    "    for data in test_data:\n",
    "        X_test.append(data[1])\n",
    "        X_test_truth.append(data[0])\n",
    "\n",
    "\n",
    "\n",
    "    return (X_train, X_test, train_label, test_label, X_train_truth, X_test_truth)\n",
    "\n",
    "\n",
    "\n",
    "def buildFeatures(train_data, bigram_dict):\n",
    "    # f1\n",
    "    length = []\n",
    "    \n",
    "    # f2\n",
    "    v_count = []\n",
    "    c_count = []\n",
    "    v_div_l = []\n",
    "    c_div_l = []\n",
    "    v_div_c = []\n",
    "    \n",
    "    # f3\n",
    "    non_alnum = []\n",
    "    non_alnum_div_l = []\n",
    "    \n",
    "    # f4\n",
    "    digit = []\n",
    "    digit_l = []\n",
    "\n",
    "    # f5\n",
    "    lower = []\n",
    "    upper = []\n",
    "    lower_div_l = []\n",
    "    upper_div_l = []\n",
    "\n",
    "    #f6\n",
    "    three_consec_cons = []\n",
    "\n",
    "    #f7\n",
    "    alpha_num = []\n",
    "\n",
    "    #f8\n",
    "    six_consec_cons = []\n",
    "\n",
    "    #f9\n",
    "    infix = []\n",
    "\n",
    "    #f10\n",
    "    bigram = []\n",
    "\n",
    "    #f11\n",
    "    most_freq = []\n",
    "\n",
    "    #f12\n",
    "    non_div_alpha = []\n",
    "\n",
    "    for word in train_data:\n",
    "        length.append(f_1(word))\n",
    "        \n",
    "        v_count.append(f_2(word)[0])\n",
    "        c_count.append(f_2(word)[1])\n",
    "        v_div_l.append(f_2(word)[2])\n",
    "        c_div_l.append(f_2(word)[3])\n",
    "        v_div_c.append(f_2(word)[4])\n",
    "        \n",
    "        non_alnum.append(f_3(word)[0])\n",
    "        non_alnum_div_l.append(f_3(word)[1])\n",
    "\n",
    "        digit.append(f_4(word)[0])\n",
    "        digit_l.append(f_4(word)[1])\n",
    "\n",
    "        lower.append(f_5(word)[0])\n",
    "        upper.append(f_5(word)[1])\n",
    "        lower_div_l.append(f_5(word)[2])\n",
    "        upper_div_l.append(f_5(word)[3])\n",
    "\n",
    "        three_consec_cons.append(f_6(word))\n",
    "\n",
    "        alpha_num.append(f_7(word))\n",
    "\n",
    "        six_consec_cons.append(f_8(word))\n",
    "\n",
    "        infix.append(f_9(word))\n",
    "\n",
    "        # can change the scaling constant (third parameter)\n",
    "        bigram.append(f_10(word, bigram_dict, 10000))\n",
    "\n",
    "        most_freq.append(f_11(word))\n",
    "\n",
    "        non_div_alpha.append(f_12(word))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # create DataFrame\n",
    "\n",
    "    df = pd.DataFrame({'length': length,\n",
    "                       'num_vowels': v_count,\n",
    "                       'num_conso': c_count,\n",
    "                       'v_div_l': v_div_l,\n",
    "                       'c_div_l': c_div_l,\n",
    "                       'v_div_c': v_div_c,\n",
    "                       'non_alnum': non_alnum,\n",
    "                       'non_alnum_div_l': non_alnum_div_l,\n",
    "                       'digit': digit,\n",
    "                       'digit_l': digit_l,\n",
    "                       'lower': lower,\n",
    "                       'upper': upper,\n",
    "                       'lower_div_l': lower_div_l,\n",
    "                       'upper_div_l': upper_div_l,\n",
    "                       'three_consec_cons': three_consec_cons,\n",
    "                       'alpha_num': alpha_num,\n",
    "                       'six_consec_cons': six_consec_cons,\n",
    "                       'infix': infix,\n",
    "                       'bigram': bigram,\n",
    "                       'most_freq': most_freq,\n",
    "                       'non_div_alpha': non_div_alpha})\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def f_1(word):\n",
    "    \n",
    "    return len(word)\n",
    "\n",
    "def f_2(word):\n",
    "    l = len(word)\n",
    "    vowels = 'aeiou'\n",
    "    cons = 'bcdfghjklmnpqrstvwxyz'\n",
    "    v_count = 0\n",
    "    c_count = 0\n",
    "    \n",
    "    for c in word:\n",
    "        if c in vowels:\n",
    "            v_count += 1\n",
    "        elif c in cons:\n",
    "            c_count += 1\n",
    "\n",
    "\n",
    "    if c_count == 0:\n",
    "        return (v_count, c_count, v_count/l, c_count/l, 0)\n",
    "\n",
    "    return (v_count, c_count, v_count/l, c_count/l, v_count/c_count)\n",
    "\n",
    "def f_3(word):\n",
    "    l = len(word)\n",
    "    non_alnum = 0\n",
    "\n",
    "    for c in word:\n",
    "        if not c.isalnum():\n",
    "            non_alnum += 1\n",
    "\n",
    "    return (non_alnum, non_alnum/l)\n",
    "\n",
    "def f_4(word):\n",
    "    l = len(word)\n",
    "    digit = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isdigit():\n",
    "            digit += 1\n",
    "    return (digit, digit/l)\n",
    "\n",
    "def f_5(word):\n",
    "    l = len(word)\n",
    "    upper = 0 \n",
    "    lower = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isupper():\n",
    "            upper += 1\n",
    "        elif c.islower():\n",
    "            lower += 1 \n",
    "\n",
    "    return (lower, upper, lower/l, upper/l)\n",
    "\n",
    "def f_6(word):\n",
    "    l = len(word)\n",
    "    groups = groupby(word)\n",
    "    result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "\n",
    "    max_count = float('-inf')\n",
    "    for word_count in result:\n",
    "        if word_count[1] > max_count:\n",
    "            max_count = word_count[1]\n",
    "\n",
    "    if max_count >= 3:\n",
    "        return max_count/l\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_7(word):\n",
    "    l = len(word)\n",
    "    alnum = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isalnum():\n",
    "            alnum += 1\n",
    "    \n",
    "    non_alnum = l - alnum\n",
    "\n",
    "    if non_alnum > alnum:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_8(word):\n",
    "    cons = 'bcdfghjklmnpqrstvwxyz'\n",
    "    consec_cons = 0\n",
    "    max_count = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c in cons:\n",
    "            consec_cons += 1\n",
    "        else:\n",
    "            if max_count < consec_cons:\n",
    "                max_count = consec_cons\n",
    "            consec_cons = 0\n",
    "    if max_count == 0:\n",
    "        max_count = consec_cons\n",
    "\n",
    "    if max_count >= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_9(word):\n",
    "    infix = word[1:-1]\n",
    "    non_alnum = 0\n",
    "    \n",
    "    for c in infix:\n",
    "        if not c.isalnum():\n",
    "            non_alnum += 1\n",
    "    if non_alnum >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_10(word, bigram_dict, c = 10000):\n",
    "\n",
    "    word = word.lower()\n",
    "    count = 0.0\n",
    "    naturalness = 0.0\n",
    "    for i in range(len(word)-1):\n",
    "        count += 1.0\n",
    "        naturalness += bigram_dict[(word[i], word[i+1])] / c\n",
    "\n",
    "    if count == 0.0:\n",
    "        return 0\n",
    "    return naturalness / count\n",
    "\n",
    "# return frequency of most frequent symbol\n",
    "def f_11(word):\n",
    "    l = len(word)\n",
    "    most_freq = collections.Counter(word).most_common(1)[0][1]\n",
    "\n",
    "    if most_freq >= 3:\n",
    "        return most_freq/l\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_12(word):\n",
    "    l = len(word)\n",
    "    alpha = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isalpha():\n",
    "            alpha += 1\n",
    "\n",
    "    non_alpha = l - alpha\n",
    "    if alpha == 0:\n",
    "        return 0\n",
    "    \n",
    "    return non_alpha / alpha\n",
    "\n",
    "def compute_bigram():\n",
    "    \n",
    "    bigram_dict = defaultdict(int)\n",
    "    truth_files_list = glob.glob('../data/ground_truth/*.txt')\n",
    "    for file in truth_files_list:\n",
    "        with open(file) as fd:\n",
    "            for line in fd:\n",
    "                each_line = line.strip().split()\n",
    "                for word in each_line:\n",
    "                    word = word.lower()\n",
    "                    for i in range(len(word)-1):\n",
    "                        bigram_dict[(word[i], word[i+1])] += 1\n",
    "\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair, words, label = labelTesseract()\n",
    "train_data, test_data, train_label, test_label, ground_truth_train, ground_truth_test = div_train(pair, label)\n",
    "\n",
    "# uncomment to test for truth, tesseract pair\n",
    "'''\n",
    "print(train_data[:10])\n",
    "print(ground_truth_train[:10])\n",
    "print(train_label[:10])\n",
    "\n",
    "print(test_data[:10])\n",
    "print(ground_truth_test[:10])\n",
    "print(test_label[:10])\n",
    "'''\n",
    "\n",
    "\n",
    "bigram_dict = compute_bigram()\n",
    "featureMatrix_train = buildFeatures(train_data, bigram_dict)\n",
    "featureMatrix_test = buildFeatures(test_data, bigram_dict)\n",
    "\n",
    "# uncomment for testing\n",
    "'''\n",
    "head = featureMatrix_train.head()\n",
    "print(head.to_string())\n",
    "'''\n",
    "\n",
    "# build classifier\n",
    "svm_class = SVC(kernel='rbf', verbose=True, gamma='scale')\n",
    "svm_class.fit(featureMatrix_train, train_label)\n",
    "\n",
    "# prediction\n",
    "prediction = svm_class.predict(featureMatrix_test)\n",
    "\n",
    "output = pd.DataFrame({'data': test_data,\n",
    "                       'label': prediction})\n",
    "\n",
    "print(output[:20])\n",
    "\n",
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(test_label, prediction))\n",
    "print(classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Output OCR to detected_typo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv('../output/detected_typo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Import detected typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$50,000.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1nclud1ng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>members</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>process</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DDT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thls</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>new</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>another</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>polltlcal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Callahan,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pennsylvanla,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>more</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dlscuss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>are</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             data  label\n",
       "0        $50,000.      0\n",
       "1       1nclud1ng      0\n",
       "2         members      1\n",
       "3             29,      0\n",
       "4             can      1\n",
       "5         process      1\n",
       "6             DDT      1\n",
       "7            thls      0\n",
       "8             new      1\n",
       "9         another      1\n",
       "10      polltlcal      0\n",
       "11           from      1\n",
       "12      Callahan,      1\n",
       "13            and      1\n",
       "14           5710      0\n",
       "15  Pennsylvanla,      0\n",
       "16            the      1\n",
       "17           more      1\n",
       "18        dlscuss      0\n",
       "19            are      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_typo = pd.read_csv('../output/detected_typo.csv',index_col = 0)\n",
    "detected_typo.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Clean detected typo (remove punctuation & number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      $50,000.\n",
       "1     1nclud1ng\n",
       "3           29,\n",
       "7          thls\n",
       "10    polltlcal\n",
       "Name: data, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_typo_and_correct = pd.read_csv('../output/detected_typo.csv',index_col = 0)\n",
    "# remove label column\n",
    "detected_typo = detected_typo_and_correct[detected_typo_and_correct.label == 0].data\n",
    "detected_typo_and_correct = detected_typo_and_correct.data\n",
    "detected_typo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_punct_num(series):\n",
    "    result = series.replace(r'\\d','')\n",
    "    result = result.str.extract(r'([a-zA-Z]+)').dropna()[0]\n",
    "    result = result.str.lower()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_typo = remove_punct_num(detected_typo)\n",
    "cleaned_typo_and_correct = remove_punct_num(detected_typo_and_correct)\n",
    "# detected_typo_and_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair, words, label = labelTesseract()\n",
    "\n",
    "true_typo = pd.DataFrame(pair)\n",
    "true_typo.columns = ['correct','typo']\n",
    "for col in true_typo.columns:\n",
    "    true_typo[col] = remove_punct_num(true_typo[col])\n",
    "true_typo = true_typo[true_typo['correct'] != true_typo['typo']].dropna().reset_index(drop = True)\n",
    "true_typo.drop_duplicates(keep = 'first',inplace = True)\n",
    "true_typo = true_typo[['typo','correct']].reset_index(drop = True)\n",
    "# true_typo.set_index('typo',inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define N & V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "truth_counts = 0\n",
    "training = []\n",
    "# create a list of all .txt files\n",
    "truth_files_list = glob.glob('../data/ground_truth/*.txt')\n",
    "# reading the ground truth file\n",
    "for file in truth_files_list:\n",
    "    with open(file) as fd:\n",
    "        for line in fd:\n",
    "            each_line = re.findall(r\"[\\w']+\",line)\n",
    "            for word in each_line:\n",
    "                training.append(word)\n",
    "                truth_counts += 1\n",
    "                \n",
    "training = pd.Series(training)\n",
    "training = training.str.replace(r'\\d','').dropna()\n",
    "\n",
    "training = training.str.lower()\n",
    "training = training[training != '']\n",
    "corpus = training.unique()\n",
    "\n",
    "N = len(training)\n",
    "V = len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Find Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "from nltk import edit_distance\n",
    "\n",
    "def typo_classification(typo,correct):\n",
    "    if (len(typo) > len(correct)):\n",
    "        return 'insertion'\n",
    "    elif (len(typo) < len(correct)):\n",
    "        return 'deletion'\n",
    "    else:\n",
    "        typo_count = Counter(typo)\n",
    "        correct_count = Counter(correct)\n",
    "        if typo_count == correct_count:\n",
    "            return 'reversal'\n",
    "        else:\n",
    "            return 'substitution'\n",
    "\n",
    "def find_candidates(typo,corpus):\n",
    "    candidates = []\n",
    "    candi_type = []\n",
    "    for word in corpus:\n",
    "        ed = edit_distance(typo,word)\n",
    "        word_type = typo_classification(typo,word)\n",
    "#         if len(typo) > 4:\n",
    "#             if ed in [1,2]:\n",
    "#                 candidates.append(word)\n",
    "#                 candi_type.append(word_type)\n",
    "#         else:\n",
    "        if ((ed == 1) |((ed == 2) & (word_type == 'reversal'))):\n",
    "            candidates.append(word)\n",
    "            candi_type.append(word_type)\n",
    "    return candidates,candi_type\n",
    "\n",
    "def find_position(typo,candidates):\n",
    "    position = []\n",
    "    for corr in candidates:\n",
    "        typo_type = typo_classification(typo,corr)\n",
    "        \n",
    "        if (typo_type == 'deletion'):\n",
    "            typo += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    if corr[i] != corr[i-1]:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        position.append([typo,corr,\"#\",corr[i],i-1,typo_type])\n",
    "                        break\n",
    "                        \n",
    "                i += 1\n",
    "        elif (typo_type == 'insertion'):\n",
    "            corr += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    \n",
    "                    if typo[i] != typo[i-1]:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        break\n",
    "                    elif ((typo[i] == typo[i-1])& (typo[i] == typo[i-2])):\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-2,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        break\n",
    "                i += 1\n",
    "        elif (typo_type == 'substitution'):\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    position.append([typo,corr,typo[i],corr[i],i,typo_type])\n",
    "                    break\n",
    "                i+=1\n",
    "                \n",
    "        elif (typo_type == 'reversal'):\n",
    "            i = 0\n",
    "            while i < len(corr)-1:\n",
    "                if ((typo[i] == corr[i+1]) & (typo[i+1] == corr[i])):\n",
    "                    typo_comb = typo[i] + typo[i+1]\n",
    "                    position.append([typo,corr,typo_comb,typo_comb[::-1],i,typo_type])\n",
    "                    break\n",
    "                i +=1\n",
    "    return position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Import 4 confusion matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionsub=pd.read_csv('../data/confusion_matrix/sub_matrix.csv',index_col = 0)\n",
    "confusionadd=pd.read_csv('../data/confusion_matrix/add_matrix.csv',index_col = 0)\n",
    "confusiondel=pd.read_csv('../data/confusion_matrix/del_matrix.csv',index_col = 0)\n",
    "confusionrev=pd.read_csv('../data/confusion_matrix/rev_matrix.csv',index_col = 0) \n",
    "# corpus = set(truth_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Count bigram & 1gram & freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "def bigram(string):\n",
    "    x = []\n",
    "    for i in range(len(string)):\n",
    "        if i == len(string) - 1:\n",
    "            return x\n",
    "        else:\n",
    "            x.append(string[i] + string[i+1])\n",
    "            \n",
    "def one_gram(string):\n",
    "    return list(string)\n",
    "\n",
    "def total_freq(training,types):\n",
    "    if types == 'bigram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += bigram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'onegram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += one_gram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'freq':\n",
    "        return Counter(training)\n",
    "    \n",
    "total_freq_bigram = total = total_freq(training,types = 'bigram')\n",
    "total_freq_1gram = total = total_freq(training,types = 'onegram')\n",
    "total_freq = total = total_freq(training,types = 'freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Calculate Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction = pd.DataFrame()\n",
    "\n",
    "def probabilityfunction(correction):\n",
    "    for i in range(0,correction.shape[0]):\n",
    "        typo = correction.iloc[i,0]\n",
    "        index=correction.iloc[i,4]\n",
    "        specificword=correction.iloc[i,1]\n",
    "        if correction.iloc[i,5]=='insertion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=typo[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result =add/total\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total=len(training)\n",
    "\n",
    "                result=add/total\n",
    "\n",
    "        if correction.iloc[i,5]=='deletion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result=delt/total\n",
    "\n",
    "\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                totall=len(training)\n",
    "\n",
    "                result=delt/totall\n",
    "        if correction.iloc[i,5]=='reversal':\n",
    "\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index]\n",
    "                Y=specificword[index+1]\n",
    "                rev=confusionrev.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                result=rev/total\n",
    "\n",
    "\n",
    "        if correction.iloc[i,5]=='substitution':\n",
    "            X=correction.iloc[i,2]\n",
    "            Y=correction.iloc[i,3]\n",
    "            sub = confusionsub.loc[X,Y]\n",
    "\n",
    "            total = total_freq_1gram[Y]\n",
    "                #lis.append(total)\n",
    "            result=sub/total\n",
    "            \n",
    "        correction.loc[i,'probability of t given c'] = result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Calculate Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correction(typos):\n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "    output = []\n",
    "    no_correction = 0\n",
    "    no_correct_word = []\n",
    "\n",
    "    for typo in tqdm_notebook(typos):\n",
    "        try:\n",
    "            candidates,cand_type = find_candidates(typo,corpus)\n",
    "            correction = find_position(typo,candidates)\n",
    "            correction = pd.DataFrame(correction)\n",
    "\n",
    "            if correction.empty:  \n",
    "                output.append(typo)\n",
    "                no_correct_word.append(typo)\n",
    "                no_correction += 1\n",
    "\n",
    "            else:\n",
    "                correction.columns = ['Typo','Correction','old','new','index','type']\n",
    "                correction = correction[correction['index'] >= 0]\n",
    "\n",
    "                if len(correction) == 1:\n",
    "                    output.append(correction.loc[0,'Correction'])\n",
    "                else:\n",
    "                    # 1. calculate the prior\n",
    "\n",
    "                    freq = [] # the number of times that the proposed correction c appears in the training set\n",
    "                    for cor in correction['Correction']:\n",
    "                        freq.append(total_freq[cor])    \n",
    "\n",
    "                    N = len(training)\n",
    "                    V = len(corpus)\n",
    "\n",
    "                    prior = (pd.DataFrame(freq) + 0.5)/(N + V/2)\n",
    "\n",
    "                    correction['probability of c'] = prior\n",
    "\n",
    "                    probabilityfunction(correction)\n",
    "\n",
    "                    # 3. Calculate the posterior and find the correction that has maximum posterior\n",
    "\n",
    "                    correction['posterior'] = correction['probability of c'] * correction['probability of t given c']\n",
    "                    best = correction[correction.posterior == correction.posterior.max()].Correction.values[0]\n",
    "                    output.append(best)\n",
    "        except:\n",
    "#             print(typo)\n",
    "            output.append(typo)\n",
    "            no_correct_word.append(typo)\n",
    "            no_correction += 1\n",
    "    #         break\n",
    "    \n",
    "    return (pd.Series(output)),no_correction,no_correct_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_denominator = len(cleaned_typo)\n",
    "recall_denominator = len(cleaned_typo_and_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def vintersection(list1,list2,ngram = False):\n",
    "    list1_dict = {}\n",
    "    list2_dict = {}\n",
    "    \n",
    "    if ngram:\n",
    "        list1 = list(''.join(list1))\n",
    "        list2 = list(''.join(list2))\n",
    "\n",
    "    for i in list1:\n",
    "        list1_dict[i] = list1_dict.get(i,0) + 1\n",
    "\n",
    "    for i in list2:\n",
    "        list2_dict[i] = list2_dict.get(i,0) + 1\n",
    "        \n",
    "    result = {}\n",
    "    for key in list1_dict.keys():\n",
    "        if key in list2_dict.keys():\n",
    "            value1 = list1_dict[key]\n",
    "            value2 = list2_dict[key]\n",
    "            min_value = min(value1,value2)\n",
    "            result[key] = min_value\n",
    "    return sum(result.values())\n",
    "\n",
    "def precision(GT,OCR,ngram = False):\n",
    "    TP = vintersection(GT,OCR,ngram)\n",
    "    if ngram:\n",
    "        OCR = list(''.join(OCR))\n",
    "    return TP/len(OCR)\n",
    "\n",
    "def recall(GT,OCR,ngram = False):\n",
    "    TP = vintersection(GT,OCR)\n",
    "    if ngram:\n",
    "        GT = list(''.join(GT))\n",
    "    return TP/len(GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Case 1: Correct all typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d142cdd6f9d24ccda4f2c02602ffb6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12111), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "typos = true_typo['typo']\n",
    "correct = true_typo['correct']\n",
    "\n",
    "Correction_output,no_correction_num,no_correct_word = Correction(typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.20%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(vintersection(Correction_output,correct)/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrected rate: 39.3%\n"
     ]
    }
   ],
   "source": [
    "print('No corrected rate: {:.1%}'.format(no_correction_num/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output correction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correction_output.to_csv('../output/Correction_output_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Recall & precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(Correction_output,correct[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     communlcations\n",
       "1          companies\n",
       "2            provide\n",
       "3               this\n",
       "4             includ\n",
       "5            heavily\n",
       "6           involved\n",
       "7                  n\n",
       "8           crltlcal\n",
       "9      environmental\n",
       "10            issues\n",
       "11              sent\n",
       "12          national\n",
       "13       legislators\n",
       "14          disposal\n",
       "15              bill\n",
       "16          continue\n",
       "17                 m\n",
       "18         suparfund\n",
       "19             which\n",
       "20          detailed\n",
       "21               cma\n",
       "22     rlghtitoiknow\n",
       "23            action\n",
       "24         continues\n",
       "25             toxic\n",
       "26        prevention\n",
       "27          requires\n",
       "28         reporting\n",
       "29        nformatlon\n",
       "           ...      \n",
       "70                mm\n",
       "71          analysis\n",
       "72         headlines\n",
       "73              june\n",
       "74          reauthor\n",
       "75            gained\n",
       "76         prlaarlly\n",
       "77          ntruslon\n",
       "78               nto\n",
       "79    reconclllatlon\n",
       "80              from\n",
       "81         edltorlal\n",
       "82             might\n",
       "83       antlclpated\n",
       "84         following\n",
       "85          accident\n",
       "86           intense\n",
       "87           inltlal\n",
       "88          interest\n",
       "89         potential\n",
       "90               nci\n",
       "91            united\n",
       "92           stories\n",
       "93         questions\n",
       "94            allude\n",
       "95          adequacy\n",
       "96          exlstlng\n",
       "97         institute\n",
       "98          question\n",
       "99             their\n",
       "Length: 100, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correction_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Only consider edit distance = 1 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_typo.map(true_typo_dict)\n",
    "# cleaned_typo\n",
    "ed_1_typo = []\n",
    "from nltk import edit_distance\n",
    "for i in range(len(true_typo)):\n",
    "    typo = true_typo.loc[i,'typo']\n",
    "    correct = true_typo.loc[i,'correct']\n",
    "    if edit_distance(typo,correct) == 1:\n",
    "        ed_1_typo.append([typo,correct])\n",
    "                         \n",
    "ed_1_typo_df = pd.DataFrame(ed_1_typo)\n",
    "ed_1_typo_df.columns = ['typo','correct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac591b0236c4b0799168bac80d6aae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "typos = ed_1_typo_df['typo']\n",
    "correct = ed_1_typo_df['correct']\n",
    "\n",
    "Correction_output,no_correction_num,no_correct_word = Correction(typos[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrected rate: 2.5%\n"
     ]
    }
   ],
   "source": [
    "print('No corrected rate: {:.1%}'.format(no_correction_num/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(vintersection(Correction_output,correct[:200])/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correction_output.to_csv('../output/Correction_output_ed_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Recall & precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vintersection(Correction_output,correct)/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
